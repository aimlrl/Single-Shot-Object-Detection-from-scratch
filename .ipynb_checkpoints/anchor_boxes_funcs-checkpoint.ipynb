{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_boxes_scales(num_conv_layers=2):\n",
    "    \"\"\"This function generates different scales of anchor boxes for different convolutional layers \n",
    "    added to the pretrained convolutional base of any pretrained network. We are assuming that we have \n",
    "    added one more layer on the top of pretrained convolutional base, therefore it will result in object \n",
    "    detection on different objects on two different scales. \n",
    "    Scales : Sizes of anchor boxes in terms of Height and Width\n",
    "    \n",
    "    Parameters:\n",
    "                num_conv_layers (int) : Number of convolutional layers added to the top of pretrained \n",
    "                                        convolutional base\n",
    "    Returning:\n",
    "                different_scales (list) : Different scales of the anchor boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    scaling_factors = np.linspace(0.2,0.9,(num_conv_layers+1))\n",
    "    #The three different scales considered by our detection are given by:\n",
    "    #0.2 (the size of one anchor box will be 1/5th of the overall size of the image in both width and height)\n",
    "    #0.55 (the size of one anchor box will be almost 1/2 of the overall size of the image in both width and height)\n",
    "    #0.9 (the size of one anchor box will be 90 % of the overall size of the image in both width and height)\n",
    "    \n",
    "    different_scales = []\n",
    "    \n",
    "    for i in range(len(scaling_factors)-1):\n",
    "        scale = [scaling_factors[i],math.sqrt(scaling_factors[i]*scaling_factors[i+1])]\n",
    "        different_scales.append(scale)\n",
    "        \n",
    "    return different_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_boxes(feature_map_shape,frame_shape,multiscale_index=0,num_conv_layers=2,\n",
    "                         aspect_ratios=[1,2,0.5]):\n",
    "    \"\"\"This function generates anchor boxes on different aspect ratios for a given scale, defined by the\n",
    "    size of the feature map output by a specific convolutional layer of the network.\n",
    "    Parameters:\n",
    "                feature_map_shape(list or a tuple) : Shape of the feature map as a result of output from\n",
    "                                                     a convolutional layer\n",
    "                frame_shape (list or tuple) : Shape of the frame which is given as an input to the network\n",
    "                multiscale_index (int) : For which layer or scale of the convolutional layer, anchor boxes\n",
    "                                         need to be fetched\n",
    "                num_conv_layers (int) : Number of convolutional layers on the top of pretrained network\n",
    "                aspect_ratios (list) : Number of aspect ratios which need to be taken into consideration\n",
    "                                       for a specific scale of anchor boxes\n",
    "    Returning:\n",
    "                generated_anchor_boxes (tensor) : anchor boxes per feature map size as a result of output\n",
    "                                                  from the convolutional layer of the network\n",
    "    \"\"\"\n",
    "    #In order to generate anchor boxes for different convolutional layers, first it's determined that what\n",
    "    #will be the scale of anchor boxes for a specific convolutional layer and then apply different aspect\n",
    "    #ratios on that scale\n",
    "    different_scales = generate_anchor_boxes_scales(num_conv_layers)\n",
    "    anchor_box_scale_per_layer = different_scales[multiscale_index]\n",
    "    \n",
    "    aspect_ratios_per_anchor_box = len(aspect_ratio) + 1\n",
    "    \n",
    "    frame_height,frame_width,_ = frame_shape\n",
    "    \n",
    "    feature_map_height,feature_map_width,_ = feature_map_shape\n",
    "    \n",
    "    #Now, we are going to finally get the scaled dimensions of the input image according to the specific \n",
    "    #convolutional layer (or scale)\n",
    "    scaled_height = frame_height * anchor_box_scale_per_layer[0]\n",
    "    scaled_width = frame_width * anchor_box_scale_per_layer[0]\n",
    "    \n",
    "    #Let's try to fetch different aspect ratio dimensions for the same scaled dimensions according to the \n",
    "    #specific convolutional layer\n",
    "    \n",
    "    all_aspect_ratios_per_scale = []\n",
    "    \n",
    "    for aspect_ratio in aspect_ratios:\n",
    "        aspect_ratiod_width = scaled_width * math.sqrt(aspect_ratio)\n",
    "        aspect_ratiod_height = scaled_height / math.sqrt(aspect_ratio)\n",
    "        all_aspect_ratios_per_scale.append([aspect_ratiod_width,aspect_ratiod_height])\n",
    "        \n",
    "    #Finally, we have added all the aspect ratios according to the specific scale except for the \n",
    "    #alternative aspect ratio for 1. So, let's add alternative aspect ratio for 1. \n",
    "    \n",
    "    aspect_ratiod_width = scaled_width * anchor_box_scale_per_layer[1]\n",
    "    aspect_ratiod_height = scaled_height * anchor_box_scale_per_layer[1]\n",
    "    all_aspect_ratios_per_scale.append([aspect_ratiod_width,aspect_ratiod_height])\n",
    "    \n",
    "    all_aspect_ratios_per_scale = np.array(all_aspect_ratios_per_scale)\n",
    "    \n",
    "    anchor_box_scale_width = frame_width / feature_map_width\n",
    "    anchor_box_scale_height = frame_height / feature_map_height\n",
    "    \n",
    "    #Let's see how anchor box coordinates will be stored in the form of a tensor\n",
    "    \n",
    "    #Let's first find the x coordinate position of the top most left feature map point\n",
    "    topmost_left_x = anchor_box_scale_width * 0.5\n",
    "    \n",
    "    #Let's now determine the x coordinate poistion of the top most right feature map point\n",
    "    topmost_right_x = (feature_map_width * anchor_box_scale_width) - (0.5 * anchor_box_scale_width)\n",
    "    \n",
    "    #let's create x coordinate positions of feature map points between top most left feature map point \n",
    "    #and top most right feature map point at equally spaced intervals of anchor box width\n",
    "    \n",
    "    cx = np.linspace(topmost_left_x,topmost_right_x,feature_map_width)\n",
    "    \n",
    "    #Let's first find the y coordinate position of the top most left feature map point\n",
    "    topmost_left_y = anchor_box_scale_height * 0.5\n",
    "    \n",
    "    #Let's now determine the y coordinate poistion of the top most right feature map point\n",
    "    topmost_right_y = (feature_map_height * anchor_box_scale_height) - (0.5 * anchor_box_scale_height)\n",
    "    \n",
    "    #let's create y coordinate positions of feature map points between top most left feature map point \n",
    "    #and top most right feature map point at equally spaced intervals of anchor box height\n",
    "    \n",
    "    cy = np.linspace(topmost_left_y,topmost_right_y,feature_map_height)\n",
    "    \n",
    "    cx_grid, cy_grid = np.meshgrid(cx,cy)\n",
    "    \n",
    "    cx_grid = np.expand_dims(cx_grid,axis=-1)\n",
    "    cy_grid = np.expand_dims(cy_grid,axis=-1)\n",
    "    \n",
    "    generated_anchor_boxes = np.zeros((feature_map_width,feature_map_height,aspect_ratios_per_anchor_box,4))\n",
    "    \n",
    "    generated_anchor_boxes[:,:,:,0] = np.tile(cx_grid,reps=(1,1,aspect_ratios_per_anchor_box))\n",
    "    generated_anchor_boxes[:,:,:,1] = np.tile(cy_grid,reps=(1,1,aspect_ratios_per_anchor_box))\n",
    "    generated_anchor_boxes[:,:,:,2] = all_aspect_ratios_per_scale[:,0]\n",
    "    generated_anchor_boxes[:,:,:,3] = all_aspect_ratios_per_scale[:,1]\n",
    "    \n",
    "    return generated_anchor_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_to_center(corner_coordinates):\n",
    "    \n",
    "    \"\"\"This function will convert corner format into center format.\n",
    "    That is from (xmin,xmax,ymin,ymax) to (cx,cy,w,h)\n",
    "    \n",
    "    Parameters:\n",
    "                corner_coordinates (tensor) : Coordinates of boxes in corner format\n",
    "    \n",
    "    Returning:\n",
    "                center_coordinates (tensor) : Coordinates of boxes in center format\n",
    "    \"\"\"\n",
    "    center_coordinates = np.copy(corner_coordinates).astype(np.float)\n",
    "    \n",
    "    center_coordinates[:,:,:,0] = 0.5 *(corner_coordinates[:,:,:,1] - corner_coordinates[:,:,:,0])\n",
    "    center_coordinates[:,:,:,0] = center_coordinates[:,:,:,0] + corner_coordinates[:,:,:,0]\n",
    "    center_coordinates[:,:,:,1] = 0.5 *(corner_coordinates[:,:,:,3] - corner_coordinates[:,:,:,2])\n",
    "    center_coordinates[:,:,:,1] = center_coordinates[:,:,:,1] + corner_coordinates[:,:,:,2]\n",
    "    \n",
    "    center_coordinates[:,:,:,2] = corner_coordinates[:,:,:,1] - corner_coordinates[:,:,:,0]\n",
    "    center_coordinates[:,:,:,3] = corner_coordinates[:,:,:,3] - corner_coordinates[:,:,:,2]\n",
    "    \n",
    "    return center_coordinates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_to_corner(center_coordinates):\n",
    "    \n",
    "    \"\"\"This function will convert center format into corner format.\n",
    "    That is from (cx,cy,w,h) to (xmin,xmax,ymin,ymax)\n",
    "    \n",
    "    Parameters:\n",
    "                center_coordinates (tensor) : Coordinates of boxes in center format\n",
    "    \n",
    "    Returning:\n",
    "                corner_coordinates (tensor) : Coordinates of boxes in corner format\n",
    "    \"\"\"\n",
    "    \n",
    "    corner_coordinates = np.copy(center_coordinates).astype(np.float)\n",
    "    \n",
    "    corner_coordinates[:,:,:,0] = center_coordinates[:,:,:,0] - (0.5 * center_coordinates[:,:,:,2])\n",
    "    corner_coordinates[:,:,:,1] = center_coordinates[:,:,:,0] + (0.5 * center_coordinates[:,:,:,2])\n",
    "    corner_coordinates[:,:,:,2] = center_coordinates[:,:,:,1] - (0.5 * center_coordinates[:,:,:,3])\n",
    "    corner_coordinates[:,:,:,3] = center_coordinates[:,:,:,1] + (0.5 * center_coordinates[:,:,:,3])\n",
    "    \n",
    "    return corner_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_positive_anchor_boxes(iou,num_unique_categories,anchor_boxes,gt_info,is_normalize,\n",
    "                                   iou_threshold):\n",
    "    \"\"\"A function to fetch all the positive anchor boxes which are having good amount of overlap \n",
    "    with ground truth bounding boxes inside the frame(image). This function will calculate the normalized \n",
    "    (if is_normalize == True) offsets for all the positive anchor boxes as well as, it will also assign\n",
    "    the categories to all the positive anchor boxes which will be the categories of ground truth bounding\n",
    "    boxes.\n",
    "    \n",
    "    Parameters:\n",
    "                iou (tensor) : IoU of each anchor box with each Ground Truth bounding box\n",
    "                num_unique_categories (int) : Total number of categories in training data\n",
    "                anchor_boxes (tensor) : anchor boxes coordinates per feature map\n",
    "                gt_info (tensor) : Coordinates as well as class labels of objects present in the image\n",
    "                is_normalize (bool) : Whether to use normalization on the offsets calculated for positive\n",
    "                                      anchor boxes or not.\n",
    "                iou_threshold (float) : If this value is less than 1 then the function will go for the second\n",
    "                                        round to find out extra positive anchor boxes apart from the ones\n",
    "                                        which it has already found out which were having maximum iou with \n",
    "                                        the ground truth bounding boxes.\n",
    "    Returning:\n",
    "                positive_anchor_boxes_categories (tensor) : Tensor of categories which have been assigned to all\n",
    "                                                        the positive anchor boxes\n",
    "                positive_anchor_boxes_offsets (tensor) : Normalized (if is_normalize == True) offsets for all the\n",
    "                                                    positive anchor boxes\n",
    "    \"\"\"\n",
    "    #Let's first try to find out all the positive anchor boxes which have the highest IoU with\n",
    "    #ground truth bounding boxes among all the anchor boxes having different aspect ratios for a \n",
    "    #specific scale. \n",
    "    max_iou_anchor_boxes = np.argmax(iou,axis=0)\n",
    "    \n",
    "    #if the IoU threshold is less than 1 then we will go for second round of selection of credible positive\n",
    "    #anchor boxes whicn have IoU with ground truth bounding boxes greater than the user selected IoU. \n",
    "    if iou_threshold < 1:\n",
    "        secondary_pos_anchor_boxes = np.argwhere(iou > iou_threshold)\n",
    "        \n",
    "        #If we are getting some number of secondary positive anchor boxes then we have to determien their \n",
    "        #offsets as well as class labels (ground truth information). \n",
    "        if secondary_pos_anchor_boxes.size > 0:\n",
    "            \n",
    "            secondary_anchor_boxes = secondary_pos_anchor_boxes[:,0]\n",
    "            secondary_anchor_boxes_categories = secondary_pos_anchor_boxes[:,1]\n",
    "            \n",
    "            secondary_anchor_boxes_gt_info = gt_info[secondary_anchor_boxes_categories]\n",
    "            \n",
    "            #Collecting all the primary as well as the secondary positive anchor boxes over an image\n",
    "            all_positive_anchor_boxes = np.concatenate([max_iou_anchor_boxes,secondary_anchor_boxes],axis=0)\n",
    "            \n",
    "            #Collecting the ground truth bounding box coordinates as well as ground truth class labels for \n",
    "            #all the positive anchor boxes to calculate the offsets for all the positive anchor boxes. \n",
    "            all_positive_anchor_boxes_gt_info = np.concatenate([gt_info,secondary_anchor_boxes_gt_info],axis=0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #If we didn't get any secondary positive anchor boxes while comparing with IoU threshold then\n",
    "            # our all positive anchor boxes will be simpy primary anchor boxes (max_iou_anchor_boxes)\n",
    "            all_positive_anchor_boxes = max_iou_anchor_boxes\n",
    "            \n",
    "            #If we didn't get any secondary positive anchor boxes while comparing with IoU threshold then\n",
    "            #the gt_info of all positive anchor boxes will be equal to the gt_info of all the primary anchor \n",
    "            #boxes\n",
    "            all_positive_anchor_boxes_gt_info = gt_info\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        all_positive_anchor_boxes = max_iou_anchor_boxes\n",
    "        all_positive_anchor_boxes_gt_info = gt_info\n",
    "        \n",
    "    positive_anchor_boxes_categories = np.zeros((iou.shape[0],num_unique_categories))\n",
    "    positive_anchor_boxes_categories[:,0] = 1\n",
    "    positive_anchor_boxes_categories[all_positive_anchor_boxes,0] = 0\n",
    "    \n",
    "    all_positive_anchor_boxes = all_positive_anchor_boxes.reshape(all_positive_anchor_boxes.shape[0],1)\n",
    "    all_positive_anchor_boxes_categories = all_positive_anchor_boxes_gt_info[:,4].reshape(\n",
    "                                            all_positive_anchor_boxes_gt_info.shape[0],1)\n",
    "    \n",
    "    row_col = np.append(all_positive_anchor_boxes,all_positive_anchor_boxes_categories,axis=1)\n",
    "    positive_anchor_boxes_categories[row_col[:,0],row_col[:,1]] = 1\n",
    "    \n",
    "    positive_anchor_boxes_offsets = np.zeros((iou.shape[0],4))\n",
    "    \n",
    "    if is_normalize == True:\n",
    "        \n",
    "        all_positive_anchor_boxes_gt_info = corner_to_center(all_positive_anchor_boxes_gt_info)\n",
    "        anchor_boxes = corner_to_center(anchor_boxes)\n",
    "        \n",
    "        offsets_xy = all_positive_anchor_boxes_gt_info[:,0:2] - anchor_boxes[all_positive_anchor_boxes,0:2]\n",
    "        offsets_xy = offsets_xy/anchor_boxes[all_positive_anchor_boxes,2:4]\n",
    "        offsets_xy = offsets_xy/0.1\n",
    "        \n",
    "        offsets_wh = np.log(all_positive_anchor_boxes_gt_info[:,2:4]/anchor_boxes[all_positive_anchor_boxes,2:4])\n",
    "        offsets_wh = offsets_wh/0.2\n",
    "        \n",
    "        offsets = np.concatenate([offsets_xy,offsets_wh],axis=1)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        offsets = all_positive_anchor_boxes_gt_info[:,0:4] - anchor_boxes[all_positive_anchor_boxes]\n",
    "        \n",
    "    positive_anchor_boxes_offsets[all_positive_anchor_boxes] = offsets\n",
    "    \n",
    "    return positive_anchor_boxes_categories,positive_anchor_boxes_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
