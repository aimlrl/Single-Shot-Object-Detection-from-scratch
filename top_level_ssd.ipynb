{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4560a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca70b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import numpy as np\n",
    "import argparse\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25535809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import output_anchor_boxes_processing\n",
    "import training_data_funcs\n",
    "import custom_data_generation\n",
    "import anchor_boxes_funcs\n",
    "import ssd_cnn\n",
    "import loss_funcs\n",
    "import ssd_command_line_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9412d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class complete_ssd:\n",
    "     \"\"\"Made of an ssd network model and a dataset generator.\n",
    "    SSD defines functions to train and validate \n",
    "    an ssd network model.\n",
    "\n",
    "    Arguments:\n",
    "        args: User-defined configurations\n",
    "\n",
    "    Attributes:\n",
    "        ssd (model): SSD network model\n",
    "        train_generator: Multi-threaded data generator for training\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"Copy user-defined configs.\n",
    "        Build backbone and ssd network models.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.ssd = None\n",
    "        self.train_generator = None\n",
    "        self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build backbone and SSD models.\"\"\"\n",
    "        # store in a dictionary the list of image files and labels\n",
    "        self.build_dictionary()\n",
    "        \n",
    "        # input shape is (480, 640, 3) by default\n",
    "        self.input_shape = (self.args.height, \n",
    "                            self.args.width,\n",
    "                            self.args.channels)\n",
    "\n",
    "        # build the backbone network (eg ResNet50)\n",
    "        # the number of feature layers is equal to n_layers\n",
    "        # feature layers are inputs to SSD network heads\n",
    "        # for class and offsets predictions\n",
    "        self.backbone = self.args.backbone(self.input_shape,\n",
    "                                           n_layers=self.args.layers)\n",
    "\n",
    "        # using the backbone, build ssd network\n",
    "        # outputs of ssd are class and offsets predictions\n",
    "        anchors, features, ssd = build_ssd(self.input_shape,\n",
    "                                           self.backbone,\n",
    "                                           n_layers=self.args.layers,\n",
    "                                           n_classes=self.n_classes)\n",
    "        # n_anchors = num of anchors per feature point (eg 4)\n",
    "        self.n_anchors = anchors\n",
    "        # feature_shapes is a list of feature map shapes\n",
    "        # per output layer - used for computing anchor boxes sizes\n",
    "        self.feature_shapes = features\n",
    "        # ssd network model\n",
    "        self.ssd = ssd\n",
    "\n",
    "\n",
    "    def build_dictionary(self):\n",
    "        \"\"\"Read input image filenames and obj detection labels\n",
    "        from a csv file and store in a dictionary.\n",
    "        \"\"\"\n",
    "        # train dataset path\n",
    "        path = os.path.join(self.args.data_path,\n",
    "                            self.args.train_labels)\n",
    "\n",
    "        # build dictionary: \n",
    "        # key=image filaname, value=box coords + class label\n",
    "        # self.classes is a list of class labels\n",
    "        self.dictionary, self.classes = build_label_dictionary(path)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.keys = np.array(list(self.dictionary.keys()))\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"Build a multi-thread train data generator.\"\"\"\n",
    "\n",
    "        self.train_generator = \\\n",
    "                DataGenerator(args=self.args,\n",
    "                              dictionary=self.dictionary,\n",
    "                              n_classes=self.n_classes,\n",
    "                              feature_shapes=self.feature_shapes,\n",
    "                              n_anchors=self.n_anchors,\n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train an ssd network.\"\"\"\n",
    "        # build the train data generator\n",
    "        if self.train_generator is None:\n",
    "            self.build_generator()\n",
    "\n",
    "        optimizer = Adam(lr=1e-3)\n",
    "        # choice of loss functions via args\n",
    "        if self.args.improved_loss:\n",
    "            print_log(\"Focal loss and smooth L1\", self.args.verbose)\n",
    "            loss = [focal_loss_categorical, smooth_l1_loss]\n",
    "        elif self.args.smooth_l1:\n",
    "            print_log(\"Smooth L1\", self.args.verbose)\n",
    "            loss = ['categorical_crossentropy', smooth_l1_loss]\n",
    "        else:\n",
    "            print_log(\"Cross-entropy and L1\", self.args.verbose)\n",
    "            loss = ['categorical_crossentropy', l1_loss]\n",
    "\n",
    "        self.ssd.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "        # model weights are saved for future validation\n",
    "        # prepare model model saving directory.\n",
    "        save_dir = os.path.join(os.getcwd(), self.args.save_dir)\n",
    "        model_name = self.backbone.name\n",
    "        model_name += '-' + str(self.args.layers) + \"layer\"\n",
    "        if self.args.normalize:\n",
    "            model_name += \"-norm\"\n",
    "        if self.args.improved_loss:\n",
    "            model_name += \"-improved_loss\"\n",
    "        elif self.args.smooth_l1:\n",
    "            model_name += \"-smooth_l1\"\n",
    "\n",
    "        if self.args.threshold < 1.0:\n",
    "            model_name += \"-extra_anchors\" \n",
    "\n",
    "        model_name += \"-\" \n",
    "        model_name += self.args.dataset\n",
    "        model_name += '-{epoch:03d}.h5'\n",
    "\n",
    "        log = \"# of classes %d\" % self.n_classes\n",
    "        print_log(log, self.args.verbose)\n",
    "        log = \"Batch size: %d\" % self.args.batch_size\n",
    "        print_log(log, self.args.verbose)\n",
    "        log = \"Weights filename: %s\" % model_name\n",
    "        print_log(log, self.args.verbose)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "        # prepare callbacks for saving model weights\n",
    "        # and learning rate scheduler\n",
    "        # learning rate decreases by 50% every 20 epochs\n",
    "        # after 60th epoch\n",
    "        checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                     verbose=1,\n",
    "                                     save_weights_only=True)\n",
    "        scheduler = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "        callbacks = [checkpoint, scheduler]\n",
    "        # train the ssd network\n",
    "        self.ssd.fit(self.train_generator,\n",
    "                     use_multiprocessing=False,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=self.args.epochs)\n",
    "\n",
    "\n",
    "    def restore_weights(self):\n",
    "        \"\"\"Load previously trained model weights\"\"\"\n",
    "        if self.args.restore_weights:\n",
    "            save_dir = os.path.join(os.getcwd(), self.args.save_dir)\n",
    "            filename = os.path.join(save_dir, self.args.restore_weights)\n",
    "            log = \"Loading weights: %s\" % filename\n",
    "            print(log, self.args.verbose)\n",
    "            self.ssd.load_weights(filename)\n",
    "\n",
    "\n",
    "    def detect_objects(self, image):\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        classes, offsets = self.ssd.predict(image)\n",
    "        image = np.squeeze(image, axis=0)\n",
    "        classes = np.squeeze(classes)\n",
    "        offsets = np.squeeze(offsets)\n",
    "        return image, classes, offsets\n",
    "\n",
    "\n",
    "    def evaluate(self, image_file=None, image=None):\n",
    "        \"\"\"Evaluate image based on image (np tensor) or filename\"\"\"\n",
    "        show = False\n",
    "        if image is None:\n",
    "            image = skimage.img_as_float(imread(image_file))\n",
    "            show = True\n",
    "\n",
    "        image, classes, offsets = self.detect_objects(image)\n",
    "        class_names, rects, _, _ = show_boxes(args,\n",
    "                                              image,\n",
    "                                              classes,\n",
    "                                              offsets,\n",
    "                                              self.feature_shapes,\n",
    "                                              show=show)\n",
    "        return class_names, rects\n",
    "\n",
    "\n",
    "    def evaluate_test(self):\n",
    "        # test labels csv path\n",
    "        path = os.path.join(self.args.data_path,\n",
    "                            self.args.test_labels)\n",
    "        # test dictionary\n",
    "        dictionary, _ = build_label_dictionary(path)\n",
    "        keys = np.array(list(dictionary.keys()))\n",
    "        # sum of precision\n",
    "        s_precision = 0\n",
    "        # sum of recall\n",
    "        s_recall = 0\n",
    "        # sum of IoUs\n",
    "        s_iou = 0\n",
    "        # evaluate per image\n",
    "        for key in keys:\n",
    "            # grounnd truth labels\n",
    "            labels = np.array(dictionary[key])\n",
    "            # 4 boxes coords are 1st four items of labels\n",
    "            gt_boxes = labels[:, 0:-1]\n",
    "            # last one is class\n",
    "            gt_class_ids = labels[:, -1]\n",
    "            # load image id by key\n",
    "            image_file = os.path.join(self.args.data_path, key)\n",
    "            image = skimage.img_as_float(imread(image_file))\n",
    "            image, classes, offsets = self.detect_objects(image)\n",
    "            # perform nms\n",
    "            _, _, class_ids, boxes = show_boxes(args,\n",
    "                                                image,\n",
    "                                                classes,\n",
    "                                                offsets,\n",
    "                                                self.feature_shapes,\n",
    "                                                show=False)\n",
    "\n",
    "            boxes = np.reshape(np.array(boxes), (-1,4))\n",
    "            # compute IoUs\n",
    "            iou = layer_utils.iou(gt_boxes, boxes)\n",
    "            # skip empty IoUs\n",
    "            if iou.size ==0:\n",
    "                continue\n",
    "            # the class of predicted box w/ max iou\n",
    "            maxiou_class = np.argmax(iou, axis=1)\n",
    "\n",
    "            # true positive\n",
    "            tp = 0\n",
    "            # false positiove\n",
    "            fp = 0\n",
    "            # sum of objects iou per image\n",
    "            s_image_iou = []\n",
    "            for n in range(iou.shape[0]):\n",
    "                # ground truth bbox has a label\n",
    "                if iou[n, maxiou_class[n]] > 0:\n",
    "                    s_image_iou.append(iou[n, maxiou_class[n]])\n",
    "                    # true positive has the same class and gt\n",
    "                    if gt_class_ids[n] == class_ids[maxiou_class[n]]:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "\n",
    "            # objects that we missed (false negative)\n",
    "            fn = abs(len(gt_class_ids) - tp - fp)\n",
    "            s_iou += (np.sum(s_image_iou) / iou.shape[0])\n",
    "            s_precision += (tp/(tp + fp))\n",
    "            s_recall += (tp/(tp + fn))\n",
    "\n",
    "\n",
    "        n_test = len(keys)\n",
    "        print_log(\"mIoU: %f\" % (s_iou/n_test),\n",
    "                  self.args.verbose)\n",
    "        print_log(\"Precision: %f\" % (s_precision/n_test),\n",
    "                  self.args.verbose)\n",
    "        print_log(\"Recall : %f\" % (s_recall/n_test),\n",
    "                  self.args.verbose)\n",
    "\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print network summary for debugging purposes.\"\"\"\n",
    "        from tensorflow.keras.utils import plot_model\n",
    "        if self.args.summary:\n",
    "            self.backbone.summary()\n",
    "            self.ssd.summary()\n",
    "            plot_model(self.backbone,\n",
    "                       to_file=\"backbone.png\",\n",
    "                       show_shapes=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ssd_parser()\n",
    "    args = parser.parse_args()\n",
    "    ssd = SSD(args)\n",
    "\n",
    "    if args.summary:\n",
    "        ssd.print_summary()\n",
    "\n",
    "    if args.restore_weights:\n",
    "        ssd.restore_weights()\n",
    "        if args.evaluate:\n",
    "            if args.image_file is None:\n",
    "                ssd.evaluate_test()\n",
    "            else:\n",
    "                ssd.evaluate(image_file=args.image_file)\n",
    "            \n",
    "    if args.train:\n",
    "        ssd.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
